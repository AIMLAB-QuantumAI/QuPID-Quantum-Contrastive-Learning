{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0719a29a-dafe-4ab5-b663-dbc3b07d9793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import json\n",
    "\n",
    "from models import (\n",
    "    ViTEncoder, QuantumEnhancer, QuPIDModel,\n",
    "    nt_xent_loss, quantum_contrastive_loss, combined_contrastive_loss\n",
    ")\n",
    "\n",
    "\n",
    "CONFIG = {\n",
    "    'training_mode': 'two_stage',\n",
    "    'checkpoint_dir': './checkpoints',\n",
    "    'results_dir': './results',\n",
    "    'image_size': 224,\n",
    "    'vit_model': 'vit_large_patch16_224',\n",
    "    'vit_dim': 1024,\n",
    "    'projection_dim': 1024,\n",
    "    'use_pretrained': True,\n",
    "    'freeze_vit_backbone': True,\n",
    "    'n_qubits': 10,\n",
    "    'n_qlayers': 3,\n",
    "    'epochs_stage1': 100,\n",
    "    'lr_stage1': 1e-4,\n",
    "    'epochs_stage2': 50,\n",
    "    'lr_stage2': 1e-3,\n",
    "    'epochs_e2e': 100,\n",
    "    'lr_e2e_vit': 1e-4,\n",
    "    'lr_e2e_quantum': 1e-3,\n",
    "    'batch_size': 32,\n",
    "    'weight_decay': 0.05,\n",
    "    'grad_clip': 1.0,\n",
    "    'temperature': 0.07,\n",
    "    'eval_every_epoch': True,\n",
    "    'save_every_n_epochs': 10,\n",
    "    'k_values': [5, 10],\n",
    "    'random_seed': 42,\n",
    "    'num_workers': 4,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "}\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def setup_directories():\n",
    "    for dir_path in [CONFIG['checkpoint_dir'], CONFIG['results_dir']]:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    return torch.device(CONFIG['device'])\n",
    "\n",
    "\n",
    "def get_ssl_transforms(image_size=224):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.RandomRotation(5),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_eval_transforms(image_size=224):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "def compute_retrieval_metrics(embeddings, labels, k_values=[5, 10]):\n",
    "    if torch.is_tensor(embeddings):\n",
    "        embeddings = embeddings.cpu().numpy()\n",
    "    if torch.is_tensor(labels):\n",
    "        labels = labels.cpu().numpy()\n",
    "    labels = np.array(labels)\n",
    "    valid_mask = labels >= 0\n",
    "    if valid_mask.sum() < 10:\n",
    "        return {f'P@{k}': 0 for k in k_values}\n",
    "    embeddings = embeddings[valid_mask]\n",
    "    labels = labels[valid_mask]\n",
    "    n_samples = len(labels)\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True) + 1e-8\n",
    "    embeddings_norm = embeddings / norms\n",
    "    sim_matrix = np.dot(embeddings_norm, embeddings_norm.T)\n",
    "    np.fill_diagonal(sim_matrix, -np.inf)\n",
    "    metrics = {}\n",
    "    for k in k_values:\n",
    "        precisions, recalls, aps, mrrs, ndcgs = [], [], [], [], []\n",
    "        for i in range(n_samples):\n",
    "            top_k_indices = np.argsort(sim_matrix[i])[::-1][:k]\n",
    "            query_label = labels[i]\n",
    "            relevant_mask = (labels == query_label)\n",
    "            relevant_mask[i] = False\n",
    "            n_relevant = relevant_mask.sum()\n",
    "            if n_relevant == 0:\n",
    "                continue\n",
    "            retrieved_relevance = (labels[top_k_indices] == query_label)\n",
    "            precision = retrieved_relevance.sum() / k\n",
    "            precisions.append(precision)\n",
    "            recall = retrieved_relevance.sum() / n_relevant\n",
    "            recalls.append(recall)\n",
    "            ap = 0\n",
    "            n_rel_so_far = 0\n",
    "            for j, rel in enumerate(retrieved_relevance):\n",
    "                if rel:\n",
    "                    n_rel_so_far += 1\n",
    "                    ap += n_rel_so_far / (j + 1)\n",
    "            if n_rel_so_far > 0:\n",
    "                ap /= min(n_relevant, k)\n",
    "            aps.append(ap)\n",
    "            first_rel_positions = np.where(retrieved_relevance)[0]\n",
    "            if len(first_rel_positions) > 0:\n",
    "                mrrs.append(1.0 / (first_rel_positions[0] + 1))\n",
    "            else:\n",
    "                mrrs.append(0)\n",
    "            dcg = sum([(1 if retrieved_relevance[j] else 0) / np.log2(j + 2) for j in range(k)])\n",
    "            ideal_rel = min(n_relevant, k)\n",
    "            idcg = sum([1 / np.log2(j + 2) for j in range(ideal_rel)])\n",
    "            ndcg = dcg / idcg if idcg > 0 else 0\n",
    "            ndcgs.append(ndcg)\n",
    "        metrics[f'P@{k}'] = np.mean(precisions) if precisions else 0\n",
    "        metrics[f'R@{k}'] = np.mean(recalls) if recalls else 0\n",
    "        metrics[f'MAP@{k}'] = np.mean(aps) if aps else 0\n",
    "        metrics[f'NDCG@{k}'] = np.mean(ndcgs) if ndcgs else 0\n",
    "    metrics['MRR'] = np.mean(mrrs) if mrrs else 0\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compute_clustering_metrics(embeddings, labels):\n",
    "    if torch.is_tensor(embeddings):\n",
    "        embeddings = embeddings.cpu().numpy()\n",
    "    if torch.is_tensor(labels):\n",
    "        labels = labels.cpu().numpy()\n",
    "    labels = np.array(labels)\n",
    "    valid_mask = labels >= 0\n",
    "    if valid_mask.sum() < 10:\n",
    "        return {'silhouette_score': 0, 'davies_bouldin': float('inf')}\n",
    "    embeddings = embeddings[valid_mask]\n",
    "    labels = labels[valid_mask]\n",
    "    unique_labels = np.unique(labels)\n",
    "    centroids = {}\n",
    "    for label in unique_labels:\n",
    "        mask = labels == label\n",
    "        if mask.sum() > 0:\n",
    "            centroids[label] = embeddings[mask].mean(axis=0)\n",
    "    intra_distances = []\n",
    "    for label in unique_labels:\n",
    "        mask = labels == label\n",
    "        if mask.sum() > 1:\n",
    "            cluster_points = embeddings[mask]\n",
    "            centroid = centroids[label]\n",
    "            distances = np.linalg.norm(cluster_points - centroid, axis=1)\n",
    "            intra_distances.extend(distances.tolist())\n",
    "    intra_cluster_dist = np.mean(intra_distances) if intra_distances else 0\n",
    "    inter_distances = []\n",
    "    centroid_list = list(centroids.values())\n",
    "    for i in range(len(centroid_list)):\n",
    "        for j in range(i + 1, len(centroid_list)):\n",
    "            dist = np.linalg.norm(centroid_list[i] - centroid_list[j])\n",
    "            inter_distances.append(dist)\n",
    "    inter_cluster_dist = np.mean(inter_distances) if inter_distances else 0\n",
    "    try:\n",
    "        silhouette = silhouette_score(embeddings, labels)\n",
    "    except:\n",
    "        silhouette = 0\n",
    "    try:\n",
    "        db_index = davies_bouldin_score(embeddings, labels)\n",
    "    except:\n",
    "        db_index = float('inf')\n",
    "    return {\n",
    "        'intra_cluster_distance': intra_cluster_dist,\n",
    "        'inter_cluster_distance': inter_cluster_dist,\n",
    "        'silhouette_score': silhouette,\n",
    "        'davies_bouldin': db_index\n",
    "    }\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, config, device):\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.history = defaultdict(list)\n",
    "    \n",
    "    def train_stage1_vit(self, vit_encoder, train_loader, val_loader=None):\n",
    "        vit_encoder = vit_encoder.to(self.device)\n",
    "        vit_encoder.train()\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            vit_encoder.projector.parameters(),\n",
    "            lr=self.config['lr_stage1'],\n",
    "            weight_decay=self.config['weight_decay']\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=self.config['epochs_stage1']\n",
    "        )\n",
    "        best_val_metric = 0\n",
    "        for epoch in range(self.config['epochs_stage1']):\n",
    "            vit_encoder.train()\n",
    "            epoch_loss = 0\n",
    "            pbar = tqdm(train_loader, desc=f\"Stage1 Epoch {epoch+1}/{self.config['epochs_stage1']}\")\n",
    "            for view1, view2, _ in pbar:\n",
    "                view1 = view1.to(self.device)\n",
    "                view2 = view2.to(self.device)\n",
    "                z1 = vit_encoder(view1)\n",
    "                z2 = vit_encoder(view2)\n",
    "                loss = nt_xent_loss(z1, z2, self.config['temperature'])\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(vit_encoder.parameters(), self.config['grad_clip'])\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            avg_loss = epoch_loss / len(train_loader)\n",
    "            scheduler.step()\n",
    "            self.history['stage1_loss'].append(avg_loss)\n",
    "            if val_loader and self.config['eval_every_epoch']:\n",
    "                val_metrics = self._evaluate_vit(vit_encoder, val_loader)\n",
    "                self.history['stage1_val_map'].append(val_metrics.get('MAP@10', 0))\n",
    "                if val_metrics.get('MAP@10', 0) > best_val_metric:\n",
    "                    best_val_metric = val_metrics.get('MAP@10', 0)\n",
    "                    self._save_checkpoint(vit_encoder, 'best_vit_encoder.pt')\n",
    "            if (epoch + 1) % self.config['save_every_n_epochs'] == 0:\n",
    "                self._save_checkpoint(vit_encoder, f'vit_encoder_epoch{epoch+1}.pt')\n",
    "        self._save_checkpoint(vit_encoder, 'final_vit_encoder.pt')\n",
    "        return vit_encoder\n",
    "    \n",
    "    def train_stage2_quantum(self, quantum_enhancer, vit_encoder, train_loader, val_loader=None):\n",
    "        vit_encoder = vit_encoder.to(self.device)\n",
    "        quantum_enhancer = quantum_enhancer.to(self.device)\n",
    "        vit_encoder.eval()\n",
    "        quantum_enhancer.train()\n",
    "        optimizer = torch.optim.Adam(\n",
    "            quantum_enhancer.parameters(),\n",
    "            lr=self.config['lr_stage2']\n",
    "        )\n",
    "        best_val_metric = 0\n",
    "        for epoch in range(self.config['epochs_stage2']):\n",
    "            quantum_enhancer.train()\n",
    "            epoch_loss = 0\n",
    "            pbar = tqdm(train_loader, desc=f\"Stage2 Epoch {epoch+1}/{self.config['epochs_stage2']}\")\n",
    "            for view1, view2, _ in pbar:\n",
    "                view1 = view1.to(self.device)\n",
    "                view2 = view2.to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    z1 = vit_encoder(view1)\n",
    "                    z2 = vit_encoder(view2)\n",
    "                q1 = quantum_enhancer(z1)\n",
    "                q2 = quantum_enhancer(z2)\n",
    "                loss = quantum_contrastive_loss(q1, q2, self.config['temperature'])\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            avg_loss = epoch_loss / len(train_loader)\n",
    "            self.history['stage2_loss'].append(avg_loss)\n",
    "            if val_loader and self.config['eval_every_epoch']:\n",
    "                val_metrics = self._evaluate_quantum(quantum_enhancer, vit_encoder, val_loader)\n",
    "                self.history['stage2_val_map'].append(val_metrics.get('MAP@10', 0))\n",
    "                if val_metrics.get('MAP@10', 0) > best_val_metric:\n",
    "                    best_val_metric = val_metrics.get('MAP@10', 0)\n",
    "                    self._save_checkpoint(quantum_enhancer, 'best_quantum_enhancer.pt')\n",
    "            if (epoch + 1) % self.config['save_every_n_epochs'] == 0:\n",
    "                self._save_checkpoint(quantum_enhancer, f'quantum_enhancer_epoch{epoch+1}.pt')\n",
    "        self._save_checkpoint(quantum_enhancer, 'final_quantum_enhancer.pt')\n",
    "        return quantum_enhancer\n",
    "    \n",
    "    def train_end_to_end(self, qupid_model, train_loader, val_loader=None):\n",
    "        qupid_model = qupid_model.to(self.device)\n",
    "        qupid_model.train()\n",
    "        optimizer = torch.optim.AdamW([\n",
    "            {'params': qupid_model.vit_encoder.projector.parameters(), \n",
    "             'lr': self.config['lr_e2e_vit']},\n",
    "            {'params': qupid_model.quantum_enhancer.parameters(), \n",
    "             'lr': self.config['lr_e2e_quantum']}\n",
    "        ], weight_decay=self.config['weight_decay'])\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=self.config['epochs_e2e']\n",
    "        )\n",
    "        best_val_metric = 0\n",
    "        for epoch in range(self.config['epochs_e2e']):\n",
    "            qupid_model.train()\n",
    "            epoch_loss = 0\n",
    "            pbar = tqdm(train_loader, desc=f\"E2E Epoch {epoch+1}/{self.config['epochs_e2e']}\")\n",
    "            for view1, view2, _ in pbar:\n",
    "                view1 = view1.to(self.device)\n",
    "                view2 = view2.to(self.device)\n",
    "                z1 = qupid_model.get_vit_embedding(view1)\n",
    "                z2 = qupid_model.get_vit_embedding(view2)\n",
    "                q1 = qupid_model.quantum_enhancer(z1)\n",
    "                q2 = qupid_model.quantum_enhancer(z2)\n",
    "                loss, _, _ = combined_contrastive_loss(\n",
    "                    z1, z2, q1, q2, \n",
    "                    temperature=self.config['temperature'],\n",
    "                    quantum_weight=0.5\n",
    "                )\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(qupid_model.parameters(), self.config['grad_clip'])\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            avg_loss = epoch_loss / len(train_loader)\n",
    "            scheduler.step()\n",
    "            self.history['e2e_loss'].append(avg_loss)\n",
    "            if val_loader and self.config['eval_every_epoch']:\n",
    "                val_metrics = self._evaluate_e2e(qupid_model, val_loader)\n",
    "                self.history['e2e_val_map'].append(val_metrics.get('MAP@10', 0))\n",
    "                if val_metrics.get('MAP@10', 0) > best_val_metric:\n",
    "                    best_val_metric = val_metrics.get('MAP@10', 0)\n",
    "                    self._save_checkpoint(qupid_model, 'best_qupid_e2e.pt')\n",
    "            if (epoch + 1) % self.config['save_every_n_epochs'] == 0:\n",
    "                self._save_checkpoint(qupid_model, f'qupid_e2e_epoch{epoch+1}.pt')\n",
    "        self._save_checkpoint(qupid_model, 'final_qupid_e2e.pt')\n",
    "        return qupid_model\n",
    "    \n",
    "    def _evaluate_vit(self, vit_encoder, dataloader):\n",
    "        vit_encoder.eval()\n",
    "        embeddings = []\n",
    "        labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                images, batch_labels = batch[0], batch[1]\n",
    "                images = images.to(self.device)\n",
    "                emb = vit_encoder(images)\n",
    "                embeddings.append(emb.cpu())\n",
    "                if torch.is_tensor(batch_labels):\n",
    "                    labels.extend(batch_labels.tolist())\n",
    "                else:\n",
    "                    labels.extend([batch_labels] if not isinstance(batch_labels, list) else batch_labels)\n",
    "        embeddings = torch.cat(embeddings, dim=0).numpy()\n",
    "        labels = np.array(labels)\n",
    "        return compute_retrieval_metrics(embeddings, labels, self.config['k_values'])\n",
    "    \n",
    "    def _evaluate_quantum(self, quantum_enhancer, vit_encoder, dataloader):\n",
    "        vit_encoder.eval()\n",
    "        quantum_enhancer.eval()\n",
    "        embeddings = []\n",
    "        labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                images, batch_labels = batch[0], batch[1]\n",
    "                images = images.to(self.device)\n",
    "                vit_emb = vit_encoder(images)\n",
    "                q_emb = quantum_enhancer(vit_emb)\n",
    "                embeddings.append(q_emb.cpu())\n",
    "                if torch.is_tensor(batch_labels):\n",
    "                    labels.extend(batch_labels.tolist())\n",
    "                else:\n",
    "                    labels.extend([batch_labels] if not isinstance(batch_labels, list) else batch_labels)\n",
    "        embeddings = torch.cat(embeddings, dim=0).numpy()\n",
    "        labels = np.array(labels)\n",
    "        return compute_retrieval_metrics(embeddings, labels, self.config['k_values'])\n",
    "    \n",
    "    def _evaluate_e2e(self, qupid_model, dataloader):\n",
    "        qupid_model.eval()\n",
    "        embeddings = []\n",
    "        labels = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                images, batch_labels = batch[0], batch[1]\n",
    "                images = images.to(self.device)\n",
    "                q_emb = qupid_model(images)\n",
    "                embeddings.append(q_emb.cpu())\n",
    "                if torch.is_tensor(batch_labels):\n",
    "                    labels.extend(batch_labels.tolist())\n",
    "                else:\n",
    "                    labels.extend([batch_labels] if not isinstance(batch_labels, list) else batch_labels)\n",
    "        embeddings = torch.cat(embeddings, dim=0).numpy()\n",
    "        labels = np.array(labels)\n",
    "        return compute_retrieval_metrics(embeddings, labels, self.config['k_values'])\n",
    "    \n",
    "    def _save_checkpoint(self, model, filename):\n",
    "        path = os.path.join(self.config['checkpoint_dir'], filename)\n",
    "        torch.save(model.state_dict(), path)\n",
    "    \n",
    "    def save_history(self, filename='training_history.json'):\n",
    "        path = os.path.join(self.config['results_dir'], filename)\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(dict(self.history), f, indent=2)\n",
    "\n",
    "\n",
    "def main(train_loader, val_loader=None):\n",
    "    set_seed(CONFIG['random_seed'])\n",
    "    setup_directories()\n",
    "    device = get_device()\n",
    "    \n",
    "    vit_encoder = ViTEncoder(\n",
    "        model_name=CONFIG['vit_model'],\n",
    "        embedding_dim=CONFIG['projection_dim'],\n",
    "        pretrained=CONFIG['use_pretrained'],\n",
    "        freeze_backbone=CONFIG['freeze_vit_backbone']\n",
    "    )\n",
    "    quantum_enhancer = QuantumEnhancer(\n",
    "        input_dim=CONFIG['projection_dim'],\n",
    "        n_qubits=CONFIG['n_qubits'],\n",
    "        n_qlayers=CONFIG['n_qlayers']\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(CONFIG, device)\n",
    "    \n",
    "    if CONFIG['training_mode'] == 'two_stage':\n",
    "        vit_encoder = trainer.train_stage1_vit(vit_encoder, train_loader, val_loader)\n",
    "        quantum_enhancer = trainer.train_stage2_quantum(\n",
    "            quantum_enhancer, vit_encoder, train_loader, val_loader\n",
    "        )\n",
    "        qupid_model = QuPIDModel(vit_encoder, quantum_enhancer)\n",
    "    elif CONFIG['training_mode'] == 'end_to_end':\n",
    "        qupid_model = QuPIDModel(vit_encoder, quantum_enhancer)\n",
    "        qupid_model = trainer.train_end_to_end(qupid_model, train_loader, val_loader)\n",
    "    \n",
    "    trainer.save_history()\n",
    "    return qupid_model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QRAG",
   "language": "python",
   "name": "qrag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
